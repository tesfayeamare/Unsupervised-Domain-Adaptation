{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tesfayeamare/Unsupervised-Domain-Adaptation/blob/main/domain_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised domain adaptation\n",
        "\n",
        "Domain adaptation is the process of adapting a model trained on one domain, called the source domain, to perform well on another domain, called the target domain.\n",
        "\n",
        "## Formal definition\n",
        "\n",
        "Given a source dataset $\\mathcal S = \\{S_i^S,y_i^S\\}_{i=1}^{N_S}$ of images and associated labes, and an unlabelled target dataset $\\mathcal T = \\{X_i^T\\}_{i=1}^{N_T}$, where $X_i\\in\\mathcal X$ and $y\\in\\mathcal Y$, $\\mathcal Y\\in\\{1,2,\\dots,K\\}$, note that $K$ is the number of object categories.\n",
        "\n",
        "The task is to learn a function $F_\\theta:\\mathcal X\\to\\mathcal Y$ with parameters $\\theta$ that maps an input image $X$ to a class label $y$ and perform well on target data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ePiAO79L4U0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mozU9PTF5BQE",
        "outputId": "bd61bd23-ae95-46c8-9691-a7b62dcc85c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfxk4lWpZnwI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from os.path import basename\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as D\n",
        "\n",
        "from torch.autograd import Function\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as M\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "RFVp_C8lxt--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/DeepLearning/runs"
      ],
      "metadata": {
        "id": "j7rZlsIFyiLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BcBeFC94qBH"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "As required, in this assignment we will be using a subset of the [Adaptiope](https://openaccess.thecvf.com/content/WACV2021/html/Ringwald_Adaptiope_A_Modern_Benchmark_for_Unsupervised_Domain_Adaptation_WACV_2021_paper.html) object recognition dataset. It follows the popular split of 80%/20% training-test split.\n",
        "\n",
        "The categories used are:\n",
        "\n",
        "* backpack\n",
        "* bookcase,\n",
        "* car jack,\n",
        "* comb,\n",
        "* crown,\n",
        "* file cabinet,\n",
        "* flat iron,\n",
        "* game controller,\n",
        "* glasses,\n",
        "* helicopter,\n",
        "* ice skates,\n",
        "* letter tray,\n",
        "* monitor,\n",
        "* mug,\n",
        "* network switch,\n",
        "* over-ear headphones,\n",
        "* pen,\n",
        "* purse,\n",
        "* stand mirror, and\n",
        "* stroller.\n",
        "\n",
        "Domains used are: *product images* and *real life*, evaluation will be performed on both direction between the domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U5YStCn4qBI"
      },
      "outputs": [],
      "source": [
        "_root_path = '/content/drive/MyDrive/DeepLearning/Adaptiope'\n",
        "\n",
        "dataset_paths = [\n",
        "    (f'{_root_path}/product_images', f'{_root_path}/real_life'),\n",
        "    (f'{_root_path}/real_life', f'{_root_path}/product_images')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers\n",
        "\n",
        "This section includes helper functions useful for training and evaluation.\n",
        "\n",
        "### Random transform\n",
        "\n",
        "Random transformations of images can be useful to improve the generalization ability of the model. By randomly transforming the training data, the model is exposed to a wider variety of variations in the input data, which can help it to learn more robust features that are not sensitive to small changes in the data. This is especially true for domain adaptation, when the model must generalize bettern than usual to perform good on the target dataset.\n",
        "\n",
        "Transformation includes:\n",
        "\n",
        "- random crop,\n",
        "- horizontal flip,\n",
        "- [automatic augments](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#augmix),\n",
        "- [color jitter](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#colorjitter), and\n",
        "- grayscale.\n",
        "\n",
        "They do not happen always for all images, but have a predefined chance of happing.\n",
        "\n",
        "#### Examples\n",
        "\n",
        "Some examples of color jitter and automatic augments:\n",
        "\n",
        "![Color jitter](https://pytorch.org/vision/stable/_images/sphx_glr_plot_transforms_006.png)\n",
        "\n",
        "![Automatic augments](https://pytorch.org/vision/stable/_images/sphx_glr_plot_transforms_023.png)\n",
        "\n",
        "### Accuracy\n",
        "\n",
        "As evaluation metric, we use the validation accuracy.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\mathit{TP} + \\mathit{TN}}{\\mathit{TP} + \\mathit{TN} + \\mathit{FP} + \\mathit{FN} }\n",
        "$$"
      ],
      "metadata": {
        "id": "XCiIhZR77l0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_cuda(dataLoader, device='cuda:0'):\n",
        "  '''\n",
        "  Move a batch from a data loader to cuda device.\n",
        "  '''\n",
        "  for batch in dataLoader:\n",
        "      for i, t in enumerate(batch):\n",
        "          batch[i] = t.to(device)\n",
        "      yield batch\n",
        "\n",
        "def calc_split(dataset, split):\n",
        "  '''\n",
        "  Used to compute the split of a dataset by giving its percentage. \n",
        "  '''\n",
        "  assert split[0] + split[1] == 1, 'Sum of the split must be 1'\n",
        "\n",
        "  l = len(dataset)\n",
        "  return math.floor(l * split[0]), math.ceil(l * split[1])\n",
        "\n",
        "def get_randomTransforms(chance=0.2):\n",
        "  '''\n",
        "  Return a composition of random transformation with custom chance. If zero\n",
        "  return an empty transformation.\n",
        "  '''\n",
        "  assert chance >= 0 and chance <= 1, 'Chance must be between 0 and 1'\n",
        "\n",
        "  if chance == 0:\n",
        "    return T.Compose([])\n",
        "\n",
        "  return T.Compose([\n",
        "    T.Resize(400),\n",
        "    T.RandomApply([T.RandomCrop(300)], p=chance),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomApply([T.AugMix()], p=chance),\n",
        "    T.RandomApply([T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=chance),\n",
        "    T.RandomApply([T.ColorJitter(brightness=.5, hue=.3)], p=chance),\n",
        "    T.RandomApply([T.Grayscale(3)], p=chance),\n",
        "  ])\n",
        "\n",
        "def get_dataLoaders(batch_size, path_source, path_target, source_transform, target_transform):\n",
        "  '''\n",
        "  Given the source and target path (and their respectively transforms) returns\n",
        "  data loader objects for training and testing.\n",
        "  '''\n",
        "\n",
        "  ds_source = torchvision.datasets.ImageFolder(\n",
        "    root=path_source, transform=source_transform)\n",
        "  ds_target = torchvision.datasets.ImageFolder(\n",
        "    root=path_target, transform=target_transform)\n",
        "  \n",
        "  ds_target_train, ds_target_test = D.random_split(\n",
        "    ds_target, calc_split(ds_target, [0.8, 0.2]))\n",
        "\n",
        "  return (\n",
        "    D.DataLoader(\n",
        "      ds_source, batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "    D.DataLoader(\n",
        "      ds_target_train, batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "    D.DataLoader(\n",
        "      ds_target_test, batch_size=batch_size, num_workers=2)\n",
        "  )\n",
        "\n",
        "def get_accuracy(outputs, targets):\n",
        "  return (outputs.max(dim=1)[1].eq(targets).sum() / outputs.size(dim=0)).item()"
      ],
      "metadata": {
        "id": "excfxlLWisZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6EbYl3N4qBJ"
      },
      "source": [
        "## Model\n",
        "\n",
        "The model used is an adversarial neural network inspired by [Unsupervised Domain Adaptation by Backpropagation](https://arxiv.org/abs/1409.7495). The architecture includes a deep feature extractor and a deep label predictor, which together form a standard feed-forward architecture. Unsupervised domain adaptation is then achiveded by adding a domain *domain classifier* connected to the feature extractor via a *gradient reversal layer* that multiplies the gradient by a certain negative constant during the backpropagation.\n",
        "\n",
        "<center>\n",
        "  <img width=\"500px\" src=\"https://i.imgur.com/BwQZMXb.png\"></img>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The focus is on learning features that combine:\n",
        "\n",
        "*   discriminativeness, and\n",
        "*   domain-invariance.\n",
        "\n",
        "This is a achieved by jointly optimizing the the *label predictor*, that predicts the class labels and the domain classifier that discriminates between the source and target domain.\n",
        "\n",
        "One important point of this idea is that the parameters of the underlying deep feature mapping are optimized in order to minimize the loss of the label classifier and to maximize the loss of the domain classifier. The latter encourages domain-invariant features to emerge in the course of the optimization.\n",
        "\n",
        "In particular we wanted to test how it is possible to adapt the architecture proposed in the paper to different pre-trained networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIlLRP9oo8K6"
      },
      "source": [
        "### Gradient reversal\n",
        "\n",
        "During the forward propagation, the layer acts as an identity transform. Instead during the backpropagation it takes the gradient from the subsequent level (domain classifier), and multiplies it by $-\\lambda$. This allows to *maximize* the error on the domain classfier instead of minimizing it.\n",
        "\n",
        "It is achieved by extending the [PyTorch autograd](https://pytorch.org/docs/stable/autograd.html) function.\n",
        "\n",
        "#### Lambda\n",
        "\n",
        "During the training the meta parameter $\\lambda$ controls the trade-off between the two objectives that shape the features during learning (class classification and domain classification). It is a value that gradually change from 0 to 1, and is useful to supress the noisy signal from the domain classifier at early stage of the training procedure.\n",
        "\n",
        "It is computed in the following way: \n",
        "\n",
        "$$\n",
        "\\lambda_p = \\frac{2}{1+\\exp(-\\gamma\\cdot p)} -1.\n",
        "$$\n",
        "\n",
        "Where $\\gamma$ was set to 10, and $p$ is the current progress.\n",
        "\n",
        "![Lambda](https://i.imgur.com/HLJNMCG.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpPOP_MJ4qBK"
      },
      "outputs": [],
      "source": [
        "class GradientReversal(Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, l):\n",
        "\n",
        "    ctx.l = l\n",
        "    \n",
        "    return x\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \n",
        "    output =  - ctx.l * grad_output\n",
        "\n",
        "    return output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UDAB\n",
        "\n",
        "We wanted to provide a framework as universal as possible. Hence the UDAB module takes as input the class classifier `y` and the domain classifier `d`. This are custom layers that should be designed with the architecture of the feature extractor in mind. For instance ResNet will prefer a flatter class classifier instead of AlexNet."
      ],
      "metadata": {
        "id": "gpuAUvTZJzV-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOPL2U2E4qBK"
      },
      "outputs": [],
      "source": [
        "class UDAB(nn.Module):\n",
        "  def __init__(self, y, d):\n",
        "    super(UDAB, self).__init__()\n",
        "    \n",
        "    self.l = 1.0\n",
        "    self.y = y\n",
        "    self.d = d\n",
        "  \n",
        "  def forward(self, x):\n",
        "    y = self.y(x)\n",
        "    d = self.d(GradientReversal.apply(torch.clone(x), self.l))\n",
        "    \n",
        "    return y, d"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "#### Baseline\n",
        "\n",
        "Train the model supervisedly on the source domain, and evaluate it, as it is, on the target domain. This passage is called **source only version** or baseline. Note that for our model this is simply accomplished by ignoring the domain classifier branch and by feeding at trainign time only source domain images and labels.\n",
        "\n",
        "#### Domain Adaptation\n",
        "\n",
        "The next **domain adaptation** part will enable the UDAB pluging by training also on the unlabelled target dataset. Hence here the loss of the domain classifier branch is used.\n",
        "\n",
        "### Step-by-step\n",
        "\n",
        "A summary of the training and evaluation process:\n",
        "\n",
        "1. Train a model baseline version, also called source only, using fully the source domain. Evaluate on the target domain.\n",
        "2. Train a model domain adaptation version, using fully the source domain and the images of the target domain, but not the labels. Evaluate on the target domain.\n",
        "3. Compare the perfomance between the two versions.\n",
        "\n",
        "Each step is repeated for every dataset (P to RW and RW to P) and for two different pre-trained network: ResNet18 and EfficientNet.\n",
        "\n"
      ],
      "metadata": {
        "id": "KC03AB5NKrQQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vopVAX04qBL"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "The `get_optimizer` is used to change the learning rate of the Adam optimizer for specific layers of the model. It takes as input a model, a list of fast layers, and two learning rates (`lr_fast` and `lr_slow`). The layers marked as fast are trained with a faster learning rate, this usually corresponds to the fully connected classification layers at the end of the pre-trained model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VRzmVD4p4qBM"
      },
      "outputs": [],
      "source": [
        "def _startswith_all(name, values):\n",
        "  for value in values:\n",
        "    if name.startswith(value):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def get_optimizer(model, fast_layers, lr_fast, lr_slow):\n",
        "  layers = ([], [])\n",
        "  \n",
        "  print(f'Learning rates: fast {lr_fast}, slow {lr_slow}')\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    #print(name)\n",
        "    if _startswith_all(name, fast_layers):\n",
        "      print(f'Fast layer: {name}')\n",
        "      layers[0].append(param) # fast\n",
        "    else:\n",
        "      layers[1].append(param) # slow\n",
        "          \n",
        "  print('Other layers will be set to slow')\n",
        "\n",
        "  return optim.Adam([\n",
        "    { 'params': layers[0], 'lr': lr_fast },\n",
        "    { 'params': layers[1], 'lr': lr_slow } \n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vY4DfioY4qBM"
      },
      "outputs": [],
      "source": [
        "def _get_d_source(size, value):\n",
        "  '''\n",
        "  Return the domain result tensor.\n",
        "  '''\n",
        "  return torch.tensor(value, dtype=torch.float).repeat(size, 1).cuda()\n",
        "\n",
        "def _get_lambda(batch_idx, epoch_idx, batches, epochs):\n",
        "  '''\n",
        "  Lambda is computed given the current progression in the training phase.\n",
        "  '''\n",
        "  return 2 / (1 + math.exp(-10 * \n",
        "    ((batch_idx + epoch_idx * batches) / (epochs * batches)) )) - 1\n",
        "\n",
        "def _step_baseline(\n",
        "  model,\n",
        "  dataLoader,\n",
        "  loss_fn,\n",
        "  optimizer,\n",
        "  gradient_clipping_value=1,\n",
        "):\n",
        "  model.train()\n",
        "  \n",
        "  n = 0; sum_loss = 0; sum_accuracy = 0\n",
        "  \n",
        "  for x, y in batch_to_cuda(dataLoader):\n",
        "    # forward step\n",
        "    r, _ = model(x)\n",
        "    # calculate loss\n",
        "    loss = loss_fn(r, y)\n",
        "    # calculate gradent\n",
        "    loss.backward()\n",
        "    # apply gradient clipping (if passed)\n",
        "    if gradient_clipping_value is not None:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping_value)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # reset gradient\n",
        "    optimizer.zero_grad()\n",
        "    # compute statistics\n",
        "    n += 1\n",
        "    sum_loss += loss.item()\n",
        "    sum_accuracy += get_accuracy(r, y)\n",
        "  \n",
        "  return sum_loss / n, sum_accuracy / n\n",
        "\n",
        "def _step(\n",
        "  model,\n",
        "  epoch_idx,\n",
        "  epochs,\n",
        "  dl_source,\n",
        "  dl_target,\n",
        "  loss_fn_source_y,\n",
        "  loss_fn_target_y,\n",
        "  loss_fn_d,\n",
        "  optimizer,\n",
        "  gradient_clipping_value=1,\n",
        "):\n",
        "  model.train()\n",
        "  # compute number of batches for lambda\n",
        "  batches = min(len(dl_source), len(dl_target))\n",
        "  \n",
        "  n = 0; sum_loss = 0; sum_accuracy = 0\n",
        "  \n",
        "  for batch_idx, ((x_source, y_source), (x_target, _)) in enumerate(zip(dl_source, dl_target)):\n",
        "    # calculate variable lambda\n",
        "    model.l = _get_lambda(batch_idx, epoch_idx, batches, epochs)\n",
        "    # train on source domain with labels\n",
        "    x_source = x_source.cuda()\n",
        "    y_source = y_source.cuda()\n",
        "    r_source, d = model(x_source)\n",
        "    # calculate losses\n",
        "    loss_source_y = loss_fn_source_y(r_source, y_source)\n",
        "    loss_source_d = loss_fn_d(d, _get_d_source(d.size(dim=0), [0, 1]))\n",
        "    # train on target domain without labels\n",
        "    x_target = x_target.cuda()\n",
        "    r_target, d = model(x_target)\n",
        "    # calculate loss\n",
        "    loss_target_y = loss_fn_target_y(r_target)\n",
        "    loss_target_d = loss_fn_d(d, _get_d_source(d.size(dim=0), [1, 0]))\n",
        "    # sum losses\n",
        "    loss = loss_source_y + loss_target_y + loss_source_d + loss_target_d\n",
        "    # compute gradient\n",
        "    loss.backward()\n",
        "    # apply gradient clipping (if passed)\n",
        "    if gradient_clipping_value is not None:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping_value)\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # reset gradient\n",
        "    optimizer.zero_grad()\n",
        "    # compute statistics\n",
        "    n += 1\n",
        "    sum_loss += loss.item()\n",
        "    sum_accuracy += get_accuracy(r_source, y_source)\n",
        "  \n",
        "  return sum_loss / n, sum_accuracy / n\n",
        "\n",
        "def _eval(\n",
        "  model,\n",
        "  dataLoader,\n",
        "  loss_fn,\n",
        "):    \n",
        "  model.eval()\n",
        "  \n",
        "  n = 0; sum_loss = 0; sum_accuracy = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for x, y in batch_to_cuda(dataLoader):\n",
        "      # forward pass\n",
        "      r, _ = model(x)\n",
        "      # compute loss\n",
        "      loss = loss_fn(r, y)\n",
        "      # compute statistics\n",
        "      n += 1\n",
        "      sum_loss += loss.item()\n",
        "      sum_accuracy += get_accuracy(r, y)\n",
        "          \n",
        "  return sum_loss / n, sum_accuracy / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKdx1ggK4qBN"
      },
      "source": [
        "### Training parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss functions\n",
        "\n",
        "Cross entropy loss is used for class classification when the label is known and domain classification. Instead, for entries from the target dataset entropy loss is used. This promotes the network to pick only one class label as result."
      ],
      "metadata": {
        "id": "S8YgmcrZEl7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropyLoss(x):\n",
        "  p = F.softmax(x, dim=1)\n",
        "  q = F.log_softmax(x, dim=1)\n",
        "  return -1. * (p * q).sum(-1).mean()"
      ],
      "metadata": {
        "id": "Lc0HaRoDsSCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugS5Yv4Y4qBO"
      },
      "outputs": [],
      "source": [
        "# class losses\n",
        "loss_fn_source_y = nn.CrossEntropyLoss()\n",
        "loss_fn_target_y = entropyLoss\n",
        "\n",
        "# domain loss\n",
        "loss_fn_d = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epochs\n",
        "\n",
        "Each experiment runs on 100 epochs."
      ],
      "metadata": {
        "id": "KKn2w-gSEpUU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H4-7pbT4qBO"
      },
      "outputs": [],
      "source": [
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Learing rates\n",
        "\n",
        "Learning rate `lr_slow` is for the feature extractor part of the pre-trained network, while `lr_slow` is for the class classification and domain classification part."
      ],
      "metadata": {
        "id": "xkoZm6VAESp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz4KqjOF4qBP"
      },
      "outputs": [],
      "source": [
        "lr_fast = 1e-2\n",
        "lr_slow = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "YzSuyhTp4qBP"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "A helper function `training_loop_baseline` is invoked every time a new experiment is performed. This train the network on both datasets and save the results to tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LslVFBm54qBQ"
      },
      "outputs": [],
      "source": [
        "def training_loop_baseline(\n",
        "  model,\n",
        "  model_name,\n",
        "  batch_size,\n",
        "  optimizer_fast_layers,\n",
        "  transforms,\n",
        "  chance=0.2\n",
        "):\n",
        "  \n",
        "  for idx, (source, target) in enumerate(dataset_paths):\n",
        "    print(f'Dataset n. {idx+1}\\nBatch size: {batch_size}\\nModel: {model_name}')\n",
        "\n",
        "    start = datetime.now()\n",
        "    \n",
        "    dl_source, dl_target, _ = get_dataLoaders(\n",
        "        batch_size,\n",
        "        source, target,\n",
        "        source_transform=T.Compose([get_randomTransforms(chance), transforms]),\n",
        "        target_transform=transforms)\n",
        "    \n",
        "    optimizer = get_optimizer(model, optimizer_fast_layers, lr_fast, lr_slow)\n",
        "\n",
        "    writer = SummaryWriter(f'/content/drive/MyDrive/DeepLearning/runs/' + \n",
        "                            f'{model_name}_dataset-{idx+1}_baseline')\n",
        "\n",
        "    for epoch_idx in range(epochs):\n",
        "\n",
        "      print(f'\\rEpoch n. {epoch_idx+1:03}/{epochs:03}..', end='')\n",
        "\n",
        "      loss, accuracy = _step_baseline(\n",
        "          model,\n",
        "          dl_source,\n",
        "          loss_fn_source_y,\n",
        "          optimizer,\n",
        "          1)\n",
        "\n",
        "      writer.add_scalar('training/loss', loss, epoch_idx)\n",
        "      writer.add_scalar('training/accuracy', accuracy, epoch_idx)\n",
        "\n",
        "      loss, accuracy = _eval(\n",
        "          model,\n",
        "          dl_target,\n",
        "          loss_fn_source_y)\n",
        "\n",
        "      writer.add_scalar('validation/loss', loss, epoch_idx)\n",
        "      writer.add_scalar('validation/accuracy', accuracy, epoch_idx)\n",
        "\n",
        "  print(f'\\rTraining completed in {datetime.now() - start}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzLZ_t9u4qBQ"
      },
      "source": [
        "#### ResNet18\n",
        "\n",
        "The architecture of the UDAB plugin is inspired to the fully connected layer of ResNet18: a flat linear layer.\n",
        "\n",
        "```\n",
        "ResNet18.fc = Linear(in_features=512, out_features=1000, bias=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2RII1-Vz4qBQ"
      },
      "outputs": [],
      "source": [
        "model = M.resnet18(weights=M.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "in_features = model.fc.in_features; out_features=20\n",
        "\n",
        "UDAB_ResNet18 = UDAB(\n",
        "    y=nn.Sequential(\n",
        "      nn.Linear(in_features, out_features)\n",
        "    ),\n",
        "    d=nn.Sequential(\n",
        "      nn.Linear(in_features, 2)\n",
        "    )\n",
        ")\n",
        "\n",
        "model.fc = UDAB_ResNet18\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "training_loop_baseline(\n",
        "    model,\n",
        "    'UDAB_ResNet18_chance-0.0',\n",
        "    64,\n",
        "    ['fc.'],\n",
        "    M.ResNet18_Weights.DEFAULT.transforms(),\n",
        "    chance=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EfficientNet\n",
        "\n",
        "Also the architecture of the EfficientNet's UDAB plugin is inspired to the classifier layer of EfficientNet.\n",
        "\n",
        "```\n",
        "EfficientNet.classifier = Sequential(\n",
        "  (0): Dropout(p=0.2, inplace=True)\n",
        "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        ")\n",
        "```\n",
        "\n",
        "In this way we do not change the original network concept but just adapt the output layers to the new classification problem. This also allows to pluignthe domain classification branch."
      ],
      "metadata": {
        "id": "jkGDEQua2v1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = M.efficientnet_b0(weights=M.EfficientNet_B0_Weights.DEFAULT)\n",
        "\n",
        "in_features = model.classifier.in_features; out_features=20\n",
        "\n",
        "UDAB_EfficientNet = UDAB(\n",
        "    y=nn.Sequential(\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(in_features, out_features),\n",
        "    ),\n",
        "    d=nn.Sequential(\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(in_features, 2)\n",
        "    )\n",
        ")\n",
        "\n",
        "model.classifier = UDAB_EfficientNet()\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "training_loop_baseline(\n",
        "    model,\n",
        "    'UDAB_EfficientNet',\n",
        "    64,\n",
        "    ['fc.'],\n",
        "    M.EfficientNet_B0_Weights.DEFAULT.transforms()\n",
        ")"
      ],
      "metadata": {
        "id": "dvIxmraT23mc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b8d998-915e-4fba-9e4f-92a2ed5041f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset n. 1\n",
            "Batch size: 64\n",
            "Model: EfficientNet-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Other layers will be set to slow\n",
            "Training completed in 2:19:58.966247\n",
            "Dataset n. 2\n",
            "Batch size: 64\n",
            "Model: EfficientNet-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Other layers will be set to slow\n",
            "Training completed in 2:26:30.250352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBlKUCS94qBX"
      },
      "source": [
        "### Domain adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A helper function `training_loop_domainAdaptation` is invoked every time a new experiment is performed. This f. train the network on both datasets and save the results to tensorboard."
      ],
      "metadata": {
        "id": "5dGXeTmyE_Bi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mNrXrIq34qBX"
      },
      "outputs": [],
      "source": [
        "def training_loop_domainAdaptation(\n",
        "  model,\n",
        "  model_name,\n",
        "  batch_size,\n",
        "  optimizer_fast_layers,\n",
        "  transforms,\n",
        "):\n",
        "    \n",
        "  for idx, (source, target) in enumerate(dataset_paths):\n",
        "    print(f'Dataset n. {idx+1}\\nModel: {model_name}')\n",
        "\n",
        "    start = datetime.now()\n",
        "    \n",
        "    dl_source, dl_target_train, dl_target_test = get_dataLoaders(\n",
        "      batch_size, source, target,\n",
        "      source_transform=T.Compose([get_randomTransforms(), transforms]),\n",
        "      target_transform=transforms)\n",
        "    \n",
        "    optimizer = get_optimizer(model, optimizer_fast_layers, lr_fast, lr_slow)\n",
        "\n",
        "    writer = SummaryWriter(f'/content/drive/MyDrive/DeepLearning/runs/' + \n",
        "                            f'{model_name}_dataset-{idx+1}_domainAdapatation')\n",
        "\n",
        "    for epoch_idx in range(epochs):\n",
        "\n",
        "      print(f'\\rEpoch n. {epoch_idx+1:03}/{epochs:03}..', end='')\n",
        "\n",
        "      loss, accuracy = _step(\n",
        "        model,\n",
        "        epoch_idx,\n",
        "        epochs,\n",
        "        dl_source,\n",
        "        dl_target_train,\n",
        "        loss_fn_source_y,\n",
        "        loss_fn_target_y,\n",
        "        loss_fn_d,\n",
        "        optimizer)\n",
        "\n",
        "      writer.add_scalar('training/loss', loss, epoch_idx)\n",
        "      writer.add_scalar('training/accuracy', accuracy, epoch_idx)\n",
        "\n",
        "      loss, accuracy = _eval(\n",
        "        model,\n",
        "        dl_target_test,\n",
        "        loss_fn_source_y)\n",
        "\n",
        "      writer.add_scalar('validation/loss', loss, epoch_idx)\n",
        "      writer.add_scalar('validation/accuracy', accuracy, epoch_idx)\n",
        "\n",
        "  print(f'\\rTraining completed in {datetime.now() - start}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS4keNG04qBY"
      },
      "source": [
        "#### ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcQB9zdD4qBZ",
        "outputId": "97fe5035-ee17-49eb-9d9c-8e9b449c8e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset n. 1\n",
            "Model: resnet18-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Fast layer: fc.y.0.weight\n",
            "Fast layer: fc.y.0.bias\n",
            "Fast layer: fc.y.1.weight\n",
            "Fast layer: fc.y.1.bias\n",
            "Fast layer: fc.y.3.weight\n",
            "Fast layer: fc.y.3.bias\n",
            "Fast layer: fc.y.4.weight\n",
            "Fast layer: fc.y.4.bias\n",
            "Fast layer: fc.d.0.weight\n",
            "Fast layer: fc.d.0.bias\n",
            "Other layers will be set to slow\n",
            "Training completed in 1:45:35.919719\n",
            "Dataset n. 2\n",
            "Model: resnet18-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Fast layer: fc.y.0.weight\n",
            "Fast layer: fc.y.0.bias\n",
            "Fast layer: fc.y.1.weight\n",
            "Fast layer: fc.y.1.bias\n",
            "Fast layer: fc.y.3.weight\n",
            "Fast layer: fc.y.3.bias\n",
            "Fast layer: fc.y.4.weight\n",
            "Fast layer: fc.y.4.bias\n",
            "Fast layer: fc.d.0.weight\n",
            "Fast layer: fc.d.0.bias\n",
            "Other layers will be set to slow\n",
            "Training completed in 1:57:46.764686\n"
          ]
        }
      ],
      "source": [
        "model = M.resnet18(weights=M.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "in_features = model.fc.in_features; out_features=20\n",
        "\n",
        "UDAB_ResNet18 = UDAB(\n",
        "    y=nn.Sequential(\n",
        "      nn.Linear(in_features, out_features)\n",
        "    ),\n",
        "    d=nn.Sequential(\n",
        "      nn.Linear(in_features, 2)\n",
        "    )\n",
        ")\n",
        "\n",
        "model.fc = UDAB_ResNet18\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "training_loop_domainAdaptation(\n",
        "    model,\n",
        "    'UDAB_ResNet18',\n",
        "    64,\n",
        "    ['fc.'],\n",
        "    M.ResNet18_Weights.DEFAULT.transforms()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EfficientNet "
      ],
      "metadata": {
        "id": "0V0aIezJLT8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = M.efficientnet_b0(weights=M.EfficientNet_B0_Weights.DEFAULT)\n",
        "\n",
        "in_features = model.fc.in_features; out_features=20\n",
        "\n",
        "UDAB_EfficientNet = UDAB(\n",
        "    y=nn.Sequential(\n",
        "      nn.Linear(in_features, 256),\n",
        "      nn.BatchNorm1d(256),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(256 , 128),\n",
        "      nn.Linear(128 , out_features)\n",
        "    ),\n",
        "    d=nn.Sequential(\n",
        "      nn.Linear(in_features, 2)\n",
        "    )\n",
        ")\n",
        "\n",
        "model.classifier = UDAB_EfficientNet()\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "training_loop_domainAdaptation(\n",
        "    model,\n",
        "    'EfficientNet-UDAB',\n",
        "    64,\n",
        "    ['fc.'],\n",
        "    M.EfficientNet_B0_Weights.DEFAULT.transforms()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "5085ebf3b88b4f45b87473036fb38694",
            "158eb81c2fcd49baa32361fe509be112",
            "a1a4b051572842c6b186989d7374f2fa",
            "d94b63fa77974b77b1db1be8d06b36e6",
            "30453a6ba13b442d9353b2f9a40f622f",
            "36e16e99b85744b0839fb404d6572e28",
            "6e88bc51669245dcb14c92c50c5d7c19",
            "4f1a4b0c2d9640b39b5a46732d1c927c",
            "2f5cc90d617c4f1194e5d2075d55bd6b",
            "9db1f2972e734caa80ac8a19a489d508",
            "9205b17326124f0e85789ecb81c5d854"
          ]
        },
        "id": "uCAeeNJsLlFj",
        "outputId": "0d52ee8f-eeb0-4c3b-b29c-492a7081a84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5085ebf3b88b4f45b87473036fb38694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset n. 1\n",
            "Model: EfficientNet-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Other layers will be set to slow\n",
            "Training completed in 2:00:53.955944\n",
            "Dataset n. 2\n",
            "Model: EfficientNet-UDAB1\n",
            "Learning rates: fast 0.01, slow 1e-05\n",
            "Other layers will be set to slow\n",
            "Training completed in 2:05:08.022986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "\n",
        "The following plots are the result of training the models ResNet18 and EfficientNet with the UDAB plugin as seen before both on dataset 1 (product to real world) and dataset 2 (real world to product).\n",
        "\n",
        "![](https://i.imgur.com/P7IwBr2.png)\n",
        "\n",
        "![](https://i.imgur.com/2SQgxik.png)\n",
        "\n",
        "![](https://i.imgur.com/K0xuOiI.png)\n",
        "\n",
        "![](https://i.imgur.com/xeNAqGE.png)\n",
        "\n",
        "\n",
        "Note how dataset-1 has worse performance due the intrinsic characteristics of the dataset. I.e., it is much more easier to learn to predict product images form training on real-world object than the opposite.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Table with validation accuracy at 100 epochs:\n",
        "\n",
        "| Model        | Dataset | Baseline | Domain adaptation | Gain |\n",
        "|--------------|---------|----------|-------------------|------|\n",
        "| ResNet18     | P to RW | 0.74     | 0.83              | 0.09 |\n",
        "| ResNet18     | RW to P | 0.96     | 0.99              | 0.03 |\n",
        "| EfficientNet | P to RW | 0.76     | 0.85              | 0.09 |\n",
        "| EfficientNet | RW to P | 0.98     | 0.99              | 0.01 |\n",
        "\n",
        "EfficientNet has higher baseline values, therefore it achieve higher values on the domain adaptation part. So if the accuracy is the most important factor then EfficientNet is the best model. However the difference with ResNet18 is very small, and for challenging datasets ResNet is able to learn much faster. For instance, ResNet on dataset-1 vs. EfficientNet on dataset-1 at 20 epochs on the domain adaptation have 0.84 vs. 0.77 accuracy (see figure 1 vs. figure 3)."
      ],
      "metadata": {
        "id": "bUeAMlXuFSme"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "_vopVAX04qBL"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5085ebf3b88b4f45b87473036fb38694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_158eb81c2fcd49baa32361fe509be112",
              "IPY_MODEL_a1a4b051572842c6b186989d7374f2fa",
              "IPY_MODEL_d94b63fa77974b77b1db1be8d06b36e6"
            ],
            "layout": "IPY_MODEL_30453a6ba13b442d9353b2f9a40f622f"
          }
        },
        "158eb81c2fcd49baa32361fe509be112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36e16e99b85744b0839fb404d6572e28",
            "placeholder": "​",
            "style": "IPY_MODEL_6e88bc51669245dcb14c92c50c5d7c19",
            "value": "100%"
          }
        },
        "a1a4b051572842c6b186989d7374f2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1a4b0c2d9640b39b5a46732d1c927c",
            "max": 21444401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f5cc90d617c4f1194e5d2075d55bd6b",
            "value": 21444401
          }
        },
        "d94b63fa77974b77b1db1be8d06b36e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db1f2972e734caa80ac8a19a489d508",
            "placeholder": "​",
            "style": "IPY_MODEL_9205b17326124f0e85789ecb81c5d854",
            "value": " 20.5M/20.5M [00:00&lt;00:00, 64.0MB/s]"
          }
        },
        "30453a6ba13b442d9353b2f9a40f622f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e16e99b85744b0839fb404d6572e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e88bc51669245dcb14c92c50c5d7c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f1a4b0c2d9640b39b5a46732d1c927c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f5cc90d617c4f1194e5d2075d55bd6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9db1f2972e734caa80ac8a19a489d508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9205b17326124f0e85789ecb81c5d854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}